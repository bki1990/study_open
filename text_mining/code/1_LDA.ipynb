{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Latent Dirichlet Allocation (LDA)는 베이지안 기법을 이용해 문서에 존재 된 토픽을 추정하는 텍스트 마이닝 기법\n",
    "- LDA의 기본이 되는 Gibbs sampling 기법을 수식과 예제와 함께 살펴 보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목차<br>\n",
    "1. 기본 개념\n",
    "2. Generative Process\n",
    "3. Inference\n",
    "4. Gibbs Sampling\n",
    "5. Collapsed Gibbs Samping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 기본 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) 모델</b><br>\n",
    "- 문서에는 잠재된 보이지 않는 토픽이 존재<br>\n",
    "- 문서는 여러 가지 잠재 토픽으로 구성\n",
    "- 문서 안의 단어는 토픽에서 생성 된 것\n",
    "- 문서를 구성하는 토픽의 확률과 토픽을 구성하는 단어의 확률을 추정하는 것이 목표\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) 가정</b><br>\n",
    "- Bag of words: 단어의 순서는 중요하지 않음 ex) 나는 밥을 먹었다 <=> 먹었다 나는 밥을<br>\n",
    "- Exchangeability: Bag of words 가정을 확률적으로 표현한 것으로 모수가 주어졌을 때 Conditionally iid라는 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generative Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Notation</b><br>\n",
    "<br>\n",
    "- Vocabulary: 모든 단어의 unique한 집합(사전)\n",
    "- 단어: vocabulary에 있는 항목으로 1부터 $V$까지 index가지고 있음<br>\n",
    "- 문서: $N$개의 단어의 순열로 $\\mathbf{w}=(w_{1},..,w_{N})$로 표현 됨, 여기서 $w_{n}$은 순열에서 $n$번째 단어<br>\n",
    "- Corpus: $M$개의 문서의 집합 $D=(\\mathbf{w}_{1},...,\\mathbf{w}_{M})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) Generative Process</b><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='C:/Users/bki19/Desktop/text_mining/Smoothed_LDA.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처: https://en.wikipedia.org/wiki/File:Smoothed_LDA.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $\\theta^{(i)}=(\\theta_{1},...,\\theta_{K}) \\sim Diri(\\alpha)$  $i=1,..,M$, $\\theta^{(i)} \\in R^{K}$<br>\n",
    "=> 각 문서는 $\\theta^{(i)}$의 확률로 토픽이 구성<br>\n",
    "=> $\\theta_{1},..,\\theta_{M}$은 각 문서를 토픽이 구성하는 확률에 대한 벡터<br>\n",
    "= >$\\alpha$는 보통 symmetric (같은 값을 같고) 하고 sparse하게 설정(1이하로 설정)<br>\n",
    "모든 문서의 토픽 확률: $\\theta=\\begin{bmatrix}\n",
    "    \\theta^{(1)}\\\\\n",
    "    ...\\\\\n",
    "    \\theta^{(M)} \\\\\n",
    "\\end{bmatrix}$ $,\\mathbf{\\theta}\\in R^{MxK}$<br>\n",
    "각 문서마다 토픽의 확률: $\\theta^{(i)}=(\\theta_{1}^{(i)},...,\\theta_{k}^{(i)} )$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. $\\varphi_{k}\\sim Diri(\\beta_{k})$ $k=1,...,K$ $\\varphi_{k} \\in R^{V}$<br>\n",
    "=> 각 토픽은 vocabulary 차원의 $\\varphi$의 확률은 가진 단어로 구성<br>\n",
    "=> $\\varphi_{1},..,\\varphi_{K}$은 각 토픽을 구성하는 단어의 확률에 대한 벡터<br>\n",
    "= >$\\beta_{i}$는 보통 sparse하게 설정<br>\n",
    "모든 단어의 확률: $\\varphi=\\begin{bmatrix}\n",
    "    \\varphi_{1}\\\\\n",
    "    ...\\\\\n",
    "    \\varphi_{K} \\\\\n",
    "\\end{bmatrix}$<br>\n",
    "각 토픽마다 단어의 확률: $\\varphi_{k}=(\\beta_{k,1},...,\\beta_{k,V})  $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. $i,j$는 단어의 위치로 $i=1,..,M$ $j=1,...,N_{i}$<br>\n",
    "(a) $z^{(i)}_{j}\\sim Categorical (\\theta_{i})$<br>\n",
    "(b) $w^{(i)}_{j}\\sim Categorical (\\varphi_{z^{(i)}_{j}} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<정리><br>\n",
    "Corpus 수준에서 Dirichlet 분포를 통해 파리미터 $\\theta$와 $\\varphi$가 발생하고<br>\n",
    "$\\theta$는 문서에 대한 토픽의 확률로 Categorical 분포를 통해 문서의 토픽 $z$가 발생<br>\n",
    "문서의 토픽이 $z$면 이에 대응하는 확률 $\\varphi$로 Categorical 분포를 통해 단어가 생성<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3) Generative process 예제</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>데이터 소개 및 요약</b><br>\n",
    "<br>\n",
    "- Wikipedia에서 수집한 경제학과 스포츠에 대한 6개의 문서를 위의 과정 적용<br>\n",
    "- $V=526$: 모든 문서에 총 526 종류의 단어가 존재<br>\n",
    "- $M=6$: 총 6개의 문서 존재<br>\n",
    "- 각 문서의 길이: $N_{1}=153,N_{2}=242,N_{3}=183,N_{4}=152,N_{5}=183,N_{6}=225$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def File_Reader(PATH):\n",
    "    with open(PATH, 'r',encoding='utf-8') as file:\n",
    "        data = file.read().replace('\\n', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/bki19/Desktop/text_mining/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=File_Reader('econ1.txt')\n",
    "doc2=File_Reader('econ2.txt')\n",
    "doc3=File_Reader('econ3.txt')\n",
    "doc4=File_Reader('sns1.txt')\n",
    "doc5=File_Reader('sns2.txt')\n",
    "doc6=File_Reader('sns3.txt')\n",
    "\n",
    "Data=[doc1,doc2,doc3,doc4,doc5,doc6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff경제학은 재화나 용역의 생산과 분배'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Noun\n",
    "from konlpy.tag import Okt\n",
    "#h=Hannanum()\n",
    "h=Okt()\n",
    "Corpus=[]\n",
    "for i in Data:\n",
    "    Corpus.append(h.nouns(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(data):\n",
    "    data=[x for x in data if len(x)>=2 ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Corpus)):\n",
    "    Corpus[i]=Preprocess(Corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리:<br>\n",
    "명사만 추출<br>\n",
    "한글자 짜리 단어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "Vocab=set(Corpus[0]+Corpus[1]+Corpus[2]+Corpus[3]+Corpus[4]+Corpus[5])\n",
    "V=len(Vocab)\n",
    "M=len(Corpus)\n",
    "print(V)\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary size, 문서 수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n",
      "242\n",
      "183\n",
      "152\n",
      "183\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "for i in Corpus:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서마다 단어 수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>단어 사전 만들기</b>\n",
    "- 각 단어에 대한 index를 부여, 예를 들어 경제라는 단어는 24번으로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocab_dict=dict(zip(sorted(Vocab), range(len(Vocab))))\n",
    "Vocab_dict['경제']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus2=[]\n",
    "for j in Corpus:\n",
    "    A=[]\n",
    "    for i in j:\n",
    "        if i in Vocab_dict.keys():\n",
    "            A.append(Vocab_dict[i])\n",
    "    Corpus2.append(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 index로 바꿔줌 <br>\n",
    "['경제학','재화','용역',,...]=>[19,122,100,..,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hyperparamter 설정</b><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=2\n",
    "alpha=1\n",
    "beta=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K=2$ 2개의 토픽이 있다고 가정<br>\n",
    "$\\alpha=1$<br>\n",
    "$\\beta=1$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>생성 과정</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $\\theta^{(i)}\\sim Diri(\\alpha)$  $i=1,..,M$, $\\theta^{(i)} \\in R^{K}$<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "T=np.random.dirichlet(alpha*np.ones(K),size=M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\theta}\\in R^{MxK}=R^{6x2}$=>문서마다 토픽의 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.596965</td>\n",
       "      <td>0.403035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.154017</td>\n",
       "      <td>0.845983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.412084</td>\n",
       "      <td>0.587916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.612261</td>\n",
       "      <td>0.387739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.799363</td>\n",
       "      <td>0.200637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.788587</td>\n",
       "      <td>0.211413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic1    Topic2\n",
       "0  0.596965  0.403035\n",
       "1  0.154017  0.845983\n",
       "2  0.412084  0.587916\n",
       "3  0.612261  0.387739\n",
       "4  0.799363  0.200637\n",
       "5  0.788587  0.211413"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(T,columns=['Topic1','Topic2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서마다 할당 된 토픽의 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) $\\varphi_{k}\\sim Diri(\\beta_{k})$ $k=1,...,K$ $\\varphi_{k} \\in R^{V}$<br>\n",
    "$\\mathbf{\\varphi}\\in R^{KxV}=R^{2x146}$=>토픽마다 단어의 확률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=np.random.dirichlet(beta*np.ones(V),size=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Word3</th>\n",
       "      <th>Word4</th>\n",
       "      <th>Word5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>0.005036</td>\n",
       "      <td>0.006143</td>\n",
       "      <td>0.000431</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.000132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word1     Word2     Word3     Word4     Word5\n",
       "Topic1  0.005036  0.006143  0.000431  0.002115  0.000132\n",
       "Topic2  0.005036  0.006143  0.000431  0.002115  0.000132"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([G[0][0:5],G[0][0:5]],index=['Topic1','Topic2'],columns=['Word1','Word2','Word3','Word4','Word5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토픽을 구성하는 단어의 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) $z_{ij}\\sim Multinomial (\\theta_{i})$, $z_{ij}=1,...K$ <br>\n",
    "=> 단어가 어떤 토픽에서 왔는지<br>\n",
    "예를들어 1번 문서의 첫 번째 단어를 보면 $z_{11}\\sim Multinomial (\\theta_{1})$에서 $z_{11}=1$이 나오면 토픽 1,2 중에 1번에서 나온것으로 취급<br>\n",
    "<br>\n",
    "$w_{ij}\\sim Multinomial (\\varphi_{z_{ij}})$, $w_{ij}=1,...,V$<br>\n",
    "만약 $z_{11}$가 1번 토픽에서 sampling 됐으면 1번 토픽에 해당하는 $\\varphi_{1}=(\\varphi_{11},...,\\varphi_{146})$에서 $w_{11}$를 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=[]\n",
    "N=[]\n",
    "for i in range(M):\n",
    "    Ni=len(Corpus2[i])\n",
    "    N.append(Ni)\n",
    "    Z=[]\n",
    "    for j in range(Ni):\n",
    "        t=np.random.multinomial(1, T[i])\n",
    "        t=np.argmax(t)\n",
    "        v=np.random.multinomial(1, G[t])\n",
    "        v=np.argmax(v)\n",
    "        Z.append(v)\n",
    "    W.append(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2=[]\n",
    "for i in W:\n",
    "    ii=[]\n",
    "    for j in i:\n",
    "        ii.append(list(Vocab_dict.keys())[list(Vocab_dict.values()).index(j)])\n",
    "    W2.append(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word1</th>\n",
       "      <th>Word2</th>\n",
       "      <th>Word3</th>\n",
       "      <th>Word4</th>\n",
       "      <th>Word5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Document1</th>\n",
       "      <td>언급</td>\n",
       "      <td>근황</td>\n",
       "      <td>하버드대</td>\n",
       "      <td>다른</td>\n",
       "      <td>상표</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Document2</th>\n",
       "      <td>행사</td>\n",
       "      <td>모든</td>\n",
       "      <td>참여</td>\n",
       "      <td>여기</td>\n",
       "      <td>해당</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word1 Word2 Word3 Word4 Word5\n",
       "Document1    언급    근황  하버드대    다른    상표\n",
       "Document2    행사    모든    참여    여기    해당"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([W2[0][0:5],W2[1][0:5]],index=['Document1','Document2'],columns=['Word1','Word2','Word3','Word4','Word5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문서1과 문서 2에 생성된 단어 5개씩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 과정은 결국<br>\n",
    "- 모든 random variable $w,z,\\theta,\\varphi$의 Joint pdf $p(w,z,\\theta,\\varphi|\\alpha,\\beta)=p(\\theta|\\alpha)p(\\varphi|\\beta)p(z|\\theta)p(w|\\varphi_{z})$의 샘플링 과정\n",
    "- 하지만 $z,\\theta,\\varphi$가 관심사이고 $w$는 실제 데이터에서 주어져 있음\n",
    "- 따라서 실제 구해야 하는 것은 데이터 $w$가 주어졌을 때의 사후 분포 $p(z,\\theta,\\varphi|w,\\alpha,\\beta)$\n",
    "- 하지만 사후분포 $p(z,\\theta,\\varphi|w,\\alpha,\\beta)=\\frac{p(w,z,\\theta,\\varphi|\\alpha,\\beta)}{p(w|\\alpha,\\beta)}$로 분자의 경우 위의 과정을 통해 추정할 수 있지만 분모는 intractable하기 때문에 직접적으로 구할 수가 없음\n",
    "- 베이지안에서 가장 많이 사용하는 방법 중 하나인 Gibbs Sampling을 통해 이 문제 해결 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>4. Gibbs Sampling</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결합 분포 $p(z,\\theta,\\varphi|w,\\alpha,\\beta)$는 Gibbs sampling을 통해 추정할 수 있음<br>\n",
    "- Gibbs sampling은 위의 결합 분포의 주변부 분포<br>\n",
    "$p(\\theta|z,\\varphi,w,\\alpha,\\beta)$<br>\n",
    "$p(z|\\theta,\\varphi,w,\\alpha,\\beta)$<br>\n",
    "$p(\\varphi|z,\\theta,w,\\alpha,\\beta)$<br>를 이용하여 iterative 하게 샘플링해서 구할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) Joint pdf</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 원하는 조건부 분포를 유도하기 위해서 모든 random variable의 결합 분포를 구함<br>\n",
    "$p(w,z,\\theta,\\varphi|\\alpha,\\beta)=p(\\theta|\\alpha)p(\\varphi|\\beta)p(z|\\theta)p(w|\\varphi_{z})$<br>\n",
    "$=(\\prod_{k=1}^{k} p(\\varphi_{k}|\\beta )(\\prod_{i=1}^{N} p(\\theta^{(i)}|\\alpha)\\prod _{j=1}^{M} p(z^{(i)}_{j}  |\\theta^{(i)} )p(w^{(i)}_{j} |\\varphi, z^{(i)}_{j}  )   ) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2) Conditional pdf of $\\theta$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\theta^{(i)}|\\theta^{(-i)},z,\\varphi,w,\\alpha,\\beta)$<br>\n",
    "$\\propto   p(\\theta^{(i)}|\\alpha)\\prod _{j=1}^{M} p(z^{(i)}_{j}  |\\theta^{(i)} ) $<br>\n",
    "$= \\prod_{k=1}^{K}(\\theta^{(i)}_{k}  )^{\\alpha-1} \\prod_{j=1}^{M}\\prod_{k=1}^{K} (\\theta^{(i)}_{k}  )^{I(z_{j}^{(i)}=k)   }  $<br>\n",
    "$= \\prod_{k=1}^{K}(\\theta^{(i)}_{k}  )^{\\alpha-1+\\sum_{j=1}^{M} I(z_{j}^{(i)}=k)   }$<br>\n",
    "$\\sim Dir(\\alpha+m^{(i)}_{k})$\n",
    "\n",
    "여기서 $m^{(i)}_{k}\\equiv \\sum_{j=1}^{M} I(z_{j}^{(i)}=k) $ <br>로 $i$번째 문서에 단어가 $k$번째 토픽으로 할당된 총 숫자<br>\n",
    "$\\theta^{(-i)}$:$i$번째 문서 외 나머지 문서들의 토픽 분포<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3) Conditional pdf of $\\varphi$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(\\varphi_{k}|\\varphi_{-k},z,\\theta,w,\\alpha,\\beta)$<br>\n",
    "$\\propto p(\\varphi_{k}|\\beta) (\\prod_{i=1}^{N_{i}} \\prod_{j=1}^{M}p(w^{(i)}_{j})|\\varphi,z^{(i)}_{j})^{I(z^{(i)}_{j}=k)  })   $<br>\n",
    "$=(\\prod_{v=1}^{V}\\varphi_{k,v}^{\\beta-1} ) (\\prod_{i=1}^{N_{i}}\\prod_{j=1}^{M}\\prod_{j=1}^{M} \\prod_{v=1}^{V}  \\varphi_{k}^{I(w_{j}^{(i)}=v\\wedge z^{(i)}_{j}=k)  }             )  $<br>\n",
    "$=\\prod_{v=1}^{V}\\varphi_{k,v}^{\\beta-1+\\sum_{i=1}^{N_{i}}\\sum_{j=1}^{M} I(w_{j}^{(i)}=v\\wedge z^{(i)}_{j}=k)}$ <br>\n",
    "$\\sim Dir(\\beta+n_{k})$<br>\n",
    "<br>\n",
    "$n_{k,v} \\equiv I(w_{j}^{(i)}=v\\wedge z^{(i)}_{j}=k)$라고 하면 현재 상태에서 문서에 상관 없이 단어 $v$가 토픽$k$에 할당 된 숫자<br>\n",
    "$n_{k}=(n_{k,1}...,n_{k,v})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3) Conditional distribution of z</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(z^{(i)}_{j}=k|\\theta,\\varphi,z_{-(i,j)},w,\\alpha,\\beta)$<br>\n",
    "$\\propto p(z^{(i)}_{j}=k|\\theta^{(i)} )p(w_{j}^{(i)}  |\\varphi,z_{j}^{(i)})=\\theta_{k}^{(i)} \\varphi_{k,w_{j}^{(i)}} $<br>\n",
    "$\\sim Categorical (\\theta_{k}^{(i)} \\varphi_{k,w_{j}^{(i)}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결국 세가지의 조건부 사후 분포가 Dirichlet와 Multinomial 분포로 심플해지는 장점이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> 4) Gibbs sampling 구현</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 $\\alpha,\\beta, T$ 설정 및 $\\theta$, $\\varphi$, $Z$의 초기치 설정 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "alpha=0.1\n",
    "beta=0.1\n",
    "K=2 ##number of topics\n",
    "Iteration=2000\n",
    "\n",
    "#1. Initialize theta matrix (Documter topic distribution)\n",
    "import numpy as np\n",
    "T=np.random.dirichlet(alpha*np.ones(K),size=M)\n",
    "\n",
    "#2. Initialzie varphi matrix (Topic word distribution)\n",
    "G=np.random.dirichlet(beta*np.ones(V),size=K)\n",
    "\n",
    "#3. Initialzie Z matrix (Word topic assignment)   \n",
    "Z=[]\n",
    "for i in range(M):\n",
    "    Ni=len(Corpus2[i])\n",
    "    zz=[]\n",
    "    for j in range(Ni):\n",
    "        z=np.random.randint(K-1)\n",
    "        zz.append(z)\n",
    "    Z.append(zz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) $p(\\theta^{(i)}|\\theta^{(-i)},z,\\varphi,w,\\alpha,\\beta)$에서 $\\theta$ sampling<br>\n",
    "2) Sampling 된 $\\theta$를 이용하여 $p(z^{(i)}_{j}=k|\\theta,\\varphi,z_{-(i,j)},w,\\alpha,\\beta)$ 샘플링<br>\n",
    "3) $p(\\varphi_{k}|\\varphi_{-k},z,\\theta,w,\\alpha,\\beta)$ 샘플링<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(Iteration):\n",
    "    \n",
    "    ###1. z (word topic assignment)\n",
    "    for i in range(M):\n",
    "        for j in range(N[i]):\n",
    "            theta=np.log(T[i] )\n",
    "            word_loc=Corpus2[i][j]\n",
    "            varphi=np.log(G[:,word_loc] )\n",
    "\n",
    "            z_param=np.exp(theta+varphi)\n",
    "            z_param/= np.sum(z_param) ##normalization\n",
    "\n",
    "            Z[i][j]=np.random.multinomial(1, z_param).argmax()\n",
    "    ###2. theta (Document per topic distribution)\n",
    "    for i in range(M):\n",
    "        m = []\n",
    "        for k in range(K):\n",
    "            m.append( Z[i].count(k))\n",
    "\n",
    "        T[i, :]=np.random.dirichlet(alpha + np.array(m) )\n",
    "\n",
    "    ###3. varphi (word topic distribution)\n",
    "    for k in range(K):\n",
    "        n=np.zeros([K,V])\n",
    "\n",
    "        for i in range(M):\n",
    "            for j in range(N[i]):\n",
    "                word_loc2=Corpus2[i][j]\n",
    "                n[k,word_loc2]+=1\n",
    "        G[k, :] = np.random.dirichlet(beta + n[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.358885</td>\n",
       "      <td>0.641115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.455121</td>\n",
       "      <td>0.544879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.429780</td>\n",
       "      <td>0.570220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.570069</td>\n",
       "      <td>0.429931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.400043</td>\n",
       "      <td>0.599957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.529611</td>\n",
       "      <td>0.470389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic1    Topic2\n",
       "0  0.358885  0.641115\n",
       "1  0.455121  0.544879\n",
       "2  0.429780  0.570220\n",
       "3  0.570069  0.429931\n",
       "4  0.400043  0.599957\n",
       "5  0.529611  0.470389"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(T,columns=['Topic1','Topic2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 직관적으로는 토픽 중 하나는 경제 기사, 하나는 SNS에 대한 주제로 구성 되어야 됨\n",
    "- 하지만 두 토픽의 분포가 뚜렷하게 구분이 가지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2=[]\n",
    "\n",
    "for i in range(K):\n",
    "    W1=[]\n",
    "    JJ=G[i].argsort()[-10:][::-1]\n",
    "    for j in JJ:\n",
    "        W1.append (list(Vocab_dict.keys())[list(Vocab_dict.values()).index(j)])\n",
    "    W2.append(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>대상</td>\n",
       "      <td>경제학</td>\n",
       "      <td>사용자</td>\n",
       "      <td>활동</td>\n",
       "      <td>페이스북</td>\n",
       "      <td>경제</td>\n",
       "      <td>인간</td>\n",
       "      <td>경우</td>\n",
       "      <td>때문</td>\n",
       "      <td>연구</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>경제학</td>\n",
       "      <td>사용자</td>\n",
       "      <td>페이스북</td>\n",
       "      <td>경제</td>\n",
       "      <td>활동</td>\n",
       "      <td>사용</td>\n",
       "      <td>대상</td>\n",
       "      <td>인스타그램</td>\n",
       "      <td>생산</td>\n",
       "      <td>서비스</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1     2   3     4   5   6      7   8    9\n",
       "Topic1   대상  경제학   사용자  활동  페이스북  경제  인간     경우  때문   연구\n",
       "Topic2  경제학  사용자  페이스북  경제    활동  사용  대상  인스타그램  생산  서비스"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(W2,index=['Topic1','Topic2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 토픽을 구성하는 단어를 상위 10개씩 정렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>왜 경제와 SNS로 토픽이 명확하게 구분되지 않았을까?</b>\n",
    "- Iteration이 부족해서 아직 수렴하지 않았을 수 있음<br>\n",
    "=>$z^{(i)}_{j}$를 하나씩 sampling 해야 되서 속도가 매우 느림\n",
    "- 각 문서 길이가 충분하지 않았을 가능성<br>\n",
    "=> LDA는 문서 길이가 길어야 결과가 좋음<br>\n",
    "- Hyper parameter $\\alpha$,$\\beta$의 잘 못된 설정<br>\n",
    "=> 두 파라미터는 문서의 토픽을 명확하게 나누는데 영향을 주고 작을 수록 토픽이 명확하게 나눠짐<br>\n",
    "- 추정된 잠재된 토픽이 경제와 SNS가 아닌 다른 토픽일 가능성<br>\n",
    "=> 해석하기 어려운 방향으로 추정됐을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Collapsed Gibbs Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA에 Gibbs sampling을 적용했을 때 너무 느리다는 단점이 있었음\n",
    "- 파라미터에 대한 inference보다는 사후 분포의 직접적인 예측이 목적일 수 있음<br>\n",
    "- 이때 $\\theta$, $\\varphi$의 inerence가 관심사가 아니고 잠재 토픽 $z$만 관심사라면? \n",
    "- Collapsed Gibbs는 $\\theta$, $\\varphi$를 marginalize out하여 계산을 simple하게 만드는 방법\n",
    "- 이제 $p(z^{(i)}_{j}=k|z_{-(i,j)},w)$가 관심사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1) $z$의 추정 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(z^{(i)}_{j}=k|z_{-(i,j)},w)=\\frac{[n_{-(i,j),k}]_{w^{(i)}_{j}} +\\beta }{ \\sum_{v^{'} }[n_{-(i,j),k}]_{v^{'} } +V\\beta   }  \\frac{ [m_{-(i,j),k}]_{k}+\\alpha    }{  \\sum_{k^{'}} [m_{-(i,j),k}]_{k^{'}}+K \\alpha }  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$[n_{-(i,j),k}]_{w^{(i)}_{j}}\\in V$: Vocabulary에서 $v$번째 단어가 토픽 $k$에 할당 된 횟수<br>\n",
    "$[m_{-(i,j),k}]_{k^{'}}\\in K$: 문서 $i$번째 문서가 topic $k^{'}$에 할당 된 횟수<br>\n",
    "=> 앞부분: 단어 $w^{(i)}$가 topic $k$에 할당 될 확률<br>\n",
    "=> 뒷부분: 문서 $i$가 topic $k$에 할당 될 확률<br>\n",
    "=> z에 대한 사후 분포가 비율로 심플하게 나타나짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "alpha=0.01\n",
    "beta=0.01\n",
    "K=2 ##number of topics\n",
    "Iteration=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초기치 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_m_n = [] # topics of words for all documents\n",
    "n_m_z = np.zeros((M, K))     # document and topic\n",
    "n_z_t = np.zeros((K, V)) # Topic vocabulary\n",
    "n_z = np.zeros(K)        # word count of each topic\n",
    "\n",
    "for i in range(M):\n",
    "    z_n=[]\n",
    "    for j in range(N[i]):\n",
    "        z=np.random.randint(0,K)\n",
    "        z_n.append(z) #sample topic\n",
    "        n_m_z[i, z] += 1 #add document topic\n",
    "        ind=Corpus2[i][j]\n",
    "        n_z_t[z, ind] += 1 #add topic vocabulary\n",
    "        n_z[z] += 1\n",
    "    z_m_n.append(np.array(z_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$z$ 추정<br>\n",
    "update할 때 현재 상태는 카운트 안 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(Iteration):\n",
    "    for i in range(M):\n",
    "        for j in range(N[i]):\n",
    "            # discount for n-th word t with topic z\n",
    "            z = z_m_n[i][j]\n",
    "            n_m_z[i, z] -= 1 #document topic\n",
    "            ind=Corpus2[i][j]\n",
    "            n_z_t[z, ind] -= 1 #topic vocabulary\n",
    "            n_z[z] -= 1 #topic vocabulary\n",
    "\n",
    "            # sampling new topic\n",
    "            p_z = (n_z_t[:, ind] + beta) * (n_m_z[i] + alpha) / (n_z + V * beta)\n",
    "            new_z = np.random.multinomial(1, p_z / p_z.sum()).argmax()\n",
    "\n",
    "            # preserve the new topic and increase the counters\n",
    "            z_m_n[i][j] = new_z\n",
    "            n_m_z[i, new_z] += 1\n",
    "\n",
    "            n_z_t[new_z, ind] += 1\n",
    "            n_z[new_z] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. $\\theta$, $\\varphi$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\theta}= \\frac{ [m_{(i,j),k}]_{k}+\\alpha    }{  \\sum_{k^{'}} [m_{(i,j),k}]_{k^{'}}+K \\alpha }  $<br>\n",
    "$\\hat{\\varphi}=\\frac{[n_{(i,j),k}]_{w^{(i)}_{j}} +\\beta }{ \\sum_{v^{'} }[n_{(i,j),k}]_{v^{'} } +V\\beta   }   $\n",
    "=> $\\theta$, $\\varphi$는 iteration 없이 z를 추정하는 과정에서 계산된 것을 이용하여 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_hat=np.zeros([M,K])\n",
    "for i in range(M):\n",
    "    T_hat[i]=(n_m_z[i]+alpha) / (np.sum(n_m_z[i]) +K*alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065416</td>\n",
       "      <td>0.934584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037228</td>\n",
       "      <td>0.962772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065621</td>\n",
       "      <td>0.934379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999934</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999945</td>\n",
       "      <td>0.000055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic1    Topic2\n",
       "0  0.065416  0.934584\n",
       "1  0.037228  0.962772\n",
       "2  0.065621  0.934379\n",
       "3  0.999934  0.000066\n",
       "4  0.999945  0.000055\n",
       "5  0.999956  0.000044"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(T_hat,columns=['Topic1','Topic2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1~3번 문서는 토픽 1일 확률이 높고 4~6은 토픽 2의 확률이 매우 높음<br>\n",
    "=> 두 종류의 문서가 뚜렷하게 구분 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_hat=np.zeros([K,V])\n",
    "for j in range(K):\n",
    "    G_hat[j]=(n_z_t[j]+beta) / (np.sum(n_z_t[j]) +V*beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP=pd.DataFrame(G_hat ,columns=sorted(Vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>사용자</th>\n",
       "      <td>0.052008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>페이스북</th>\n",
       "      <td>0.031882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>활동</th>\n",
       "      <td>0.020142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>인스타그램</th>\n",
       "      <td>0.020142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>미국</th>\n",
       "      <td>0.015111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>친구</th>\n",
       "      <td>0.013434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>서비스</th>\n",
       "      <td>0.013434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>트위터</th>\n",
       "      <td>0.013434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>세계</th>\n",
       "      <td>0.011757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>통해</th>\n",
       "      <td>0.011757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Topic1\n",
       "사용자    0.052008\n",
       "페이스북   0.031882\n",
       "활동     0.020142\n",
       "인스타그램  0.020142\n",
       "미국     0.015111\n",
       "친구     0.013434\n",
       "서비스    0.013434\n",
       "트위터    0.013434\n",
       "세계     0.011757\n",
       "통해     0.011757"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP1=pd.DataFrame(TOP.iloc[0].sort_values(0, ascending=False).transpose())[0:10]\n",
    "TOP1.columns=['Topic1']\n",
    "TOP2=pd.DataFrame(TOP.iloc[1].sort_values(0, ascending=False).transpose())[0:10]\n",
    "TOP2.columns=['Topic2']\n",
    "TOP1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>경제학</th>\n",
       "      <td>0.059773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>경제</th>\n",
       "      <td>0.036233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>대상</th>\n",
       "      <td>0.028990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>연구</th>\n",
       "      <td>0.027179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>인간</th>\n",
       "      <td>0.019936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>경우</th>\n",
       "      <td>0.016315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>철수</th>\n",
       "      <td>0.016315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>민희</th>\n",
       "      <td>0.016315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>영희</th>\n",
       "      <td>0.016315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>재화</th>\n",
       "      <td>0.014504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic2\n",
       "경제학  0.059773\n",
       "경제   0.036233\n",
       "대상   0.028990\n",
       "연구   0.027179\n",
       "인간   0.019936\n",
       "경우   0.016315\n",
       "철수   0.016315\n",
       "민희   0.016315\n",
       "영희   0.016315\n",
       "재화   0.014504"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토픽을 구성하는 상위 단어를 봤을 때 토픽 1은 SNS 관련 단어가 많이 나오고 토픽 2는 경제학 관련 단어로 구성됨\n",
    "- 경제 문서에서 영희 철수와 같은 사람의 이름으로 비유를 많이 해서 이와 같은 단어가 상위 단어가 됐을 가능성이 있음\n",
    "- 고유명사 처리 및 전처리를 더 할 필요성이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Collpased Gibbs Sampling 문제점</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여전히 하이퍼 파라미터 설정에서 자유롭지 못함\n",
    "- Gibbs sampling 보다 속도는 훨씬 빠르지만 정확성은 떨어짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>출처</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.\n",
    "- Darling, W. M. (2011, December). A theoretical and practical implementation tutorial on topic modeling and - gibbs sampling. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies (pp. 642-647).\n",
    "- https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "- https://wiseodd.github.io/techblog/2017/09/07/lda-gibbs/\n",
    "- http://brooksandrew.github.io/simpleblog/articles/latent-dirichlet-allocation-under-the-hood/\n",
    "- https://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html\n",
    "- https://medium.com/@pasdan/latent-dirichlet-allocation-via-python-ef77b86dcee5\n",
    "- https://books.google.co.kr/books?id=oBV8DAAAQBAJ&pg=PA109&lpg=PA109&dq=gibbs+sampling+for+lda+conditional&source=bl&ots=bcf4nWqFy4&sig=ACfU3U2GtQ6kpGgUxA3LhN_dArrYU6tGqw&hl=en&sa=X&ved=2ahUKEwiW84et89fiAhX5yosBHYlDB5Y4HhDoATANegQICRAB#v=onepage&q=gibbs%20sampling%20for%20lda%20conditional&f=false\n",
    "- https://shuyo.wordpress.com/2011/05/31/collapsed-gibbs-sampling-estimation-for-latent-dirichlet-allocation-2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
